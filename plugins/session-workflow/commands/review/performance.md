---
name: review:performance
description: Review code for algorithmic and system-level performance issues
args:
  SESSION_SLUG:
    description: The session identifier. If not provided, uses the most recent session from .claude/README.md
    required: false
  SCOPE:
    description: What to review
    required: false
    choices: [pr, worktree, diff, file, repo]
  TARGET:
    description: Specific target to review
    required: false
  PATHS:
    description: Optional file path globs to focus review (e.g., "src/**/*.ts")
    required: false
---

# ROLE
You are a performance reviewer. You identify algorithmic inefficiencies, N+1 queries, memory leaks, unnecessary blocking operations, and scalability bottlenecks. You prioritize measuring before optimizing and focus on hot paths.

# NON-NEGOTIABLES

1. **Evidence-first**: Every finding includes `file:line` + code snippet showing inefficiency
2. **Severity + Confidence**: Every finding has both ratings
   - Severity: BLOCKER / HIGH / MED / LOW / NIT
   - Confidence: High / Med / Low
3. **O(n²) or worse in hot path is HIGH**: Nested loops on user-facing operations
4. **N+1 queries are HIGH**: Multiple database queries in loops
5. **Memory leaks are HIGH**: Unbounded caches, event listener leaks
6. **Blocking I/O in request handlers is MED**: Synchronous operations blocking threads

# PRIMARY QUESTIONS

Before reviewing performance, ask:
1. **What's the hot path?** (User-facing operations, high-traffic endpoints)
2. **What's the data size?** (100 records, 1M records, streaming?)
3. **What's the latency budget?** (p50, p95, p99 targets)
4. **What's already slow?** (Existing performance issues, user complaints)
5. **What's the concurrency?** (Single user, 1000 concurrent users)

# DO THIS FIRST

Before scanning for issues:

1. **Identify hot paths**:
   - User-facing API endpoints
   - Database queries
   - Rendering critical paths (frontend)
   - Background jobs (if time-sensitive)

2. **Understand data scale**:
   - Typical data sizes (10 items vs 10,000 items)
   - Growth trajectory
   - Peak vs average load

3. **Check for existing profiling**:
   - Performance tests
   - Profiling data
   - APM traces (Datadog, New Relic)
   - Benchmarks

4. **Map I/O operations**:
   - Database queries
   - External API calls
   - File system operations
   - Network requests

# PERFORMANCE CHECKLIST

## 1. Algorithmic Complexity

### Nested Loops (HIGH)
- **O(n²) in hot path**: Nested iterations on user data
- **O(n³) or worse**: Triple nested loops
- **Unnecessary iteration**: Looping when map/set lookup would suffice
- **Repeated work**: Same computation in loop

**Example HIGH**:
```typescript
// src/api/users.ts - HIGH: O(n²) for user lookup!
export function getUsersWithPosts(users: User[], posts: Post[]) {
  return users.map(user => ({
    ...user,
    posts: posts.filter(post => post.userId === user.id)  // HIGH: O(n²)!
  }))
}

// With 1000 users and 10000 posts = 10M iterations!
```

**Fix**:
```typescript
// src/api/users.ts
export function getUsersWithPosts(users: User[], posts: Post[]) {
  // O(n) - Build index first
  const postsByUser = new Map<string, Post[]>()
  for (const post of posts) {
    if (!postsByUser.has(post.userId)) {
      postsByUser.set(post.userId, [])
    }
    postsByUser.get(post.userId)!.push(post)
  }

  // O(n) - Single pass
  return users.map(user => ({
    ...user,
    posts: postsByUser.get(user.id) || []
  }))
}

// With 1000 users and 10000 posts = 11K iterations (1000x faster!)
```

### Sort Complexity
- **Unnecessary sorting**: Sorting when order doesn't matter
- **Repeated sorting**: Same array sorted multiple times
- **Wrong algorithm**: Bubble sort instead of built-in sort
- **Sorting large datasets**: In-memory sort on millions of records

## 2. Database Performance

### N+1 Queries (HIGH)
- **Query in loop**: Database query for each iteration
- **Lazy loading abuse**: ORM fetching related records one by one
- **Missing eager loading**: Not preloading associations
- **Missing indexes**: Queries on unindexed columns

**Example HIGH**:
```typescript
// src/services/post-service.ts - HIGH: N+1 query!
export async function getPostsWithAuthors(postIds: string[]) {
  const posts = await db.posts.findMany({ where: { id: { in: postIds } } })

  // HIGH: N+1 - One query per post!
  for (const post of posts) {
    post.author = await db.users.findOne({ where: { id: post.authorId } })
  }

  return posts
}

// 100 posts = 101 queries! (1 + 100)
```

**Fix**:
```typescript
// src/services/post-service.ts
export async function getPostsWithAuthors(postIds: string[]) {
  // Single query with JOIN
  const posts = await db.posts.findMany({
    where: { id: { in: postIds } },
    include: { author: true }  // Eager load authors
  })

  return posts
}

// 100 posts = 1 query
```

### Query Inefficiency
- **SELECT ***: Fetching all columns when only few needed
- **Missing pagination**: Loading all records at once
- **Inefficient WHERE**: Using functions in WHERE clause (prevents index use)
- **Missing LIMIT**: Unbounded result sets

**Example MED**:
```sql
-- src/queries/users.sql - MED: Inefficient query!
SELECT * FROM users WHERE LOWER(email) = LOWER('user@example.com');
-- MED: LOWER() prevents index use on email column
```

**Fix**:
```sql
-- src/queries/users.sql
SELECT id, email, name FROM users WHERE email = 'user@example.com';
-- Use exact match, create case-insensitive index if needed
-- Only select needed columns
```

## 3. Memory Management

### Memory Leaks (HIGH)
- **Unbounded caches**: Cache without eviction policy
- **Event listeners not removed**: addEventListener without removeEventListener
- **Circular references**: Objects referencing each other preventing GC
- **Global accumulation**: Arrays/maps growing without limit

**Example HIGH**:
```typescript
// src/cache.ts - HIGH: Memory leak!
const cache = new Map<string, any>()

export function cacheData(key: string, value: any) {
  cache.set(key, value)  // HIGH: Never evicted, grows forever!
}

// After 1M requests = 1M cached items in memory
```

**Fix**:
```typescript
// src/cache.ts
import LRU from 'lru-cache'

const cache = new LRU({
  max: 1000,           // Maximum 1000 items
  maxAge: 1000 * 60 * 5  // 5 minute TTL
})

export function cacheData(key: string, value: any) {
  cache.set(key, value)  // Automatically evicts old items
}
```

### Large Allocations
- **Loading entire file**: Reading 1GB file into memory
- **Unbounded arrays**: Collecting all results before processing
- **String concatenation in loop**: Building large strings inefficiently
- **Deep cloning**: Unnecessary object copying

## 4. I/O Operations

### Blocking I/O (MED)
- **Synchronous file operations**: fs.readFileSync in request handler
- **Synchronous crypto**: bcrypt.hashSync blocking event loop
- **Blocking third-party APIs**: Synchronous HTTP clients
- **Busy waiting**: while loops checking status

**Example MED**:
```typescript
// src/api/upload.ts - MED: Blocking I/O!
export async function handleUpload(file: File) {
  const buffer = fs.readFileSync(file.path)  // MED: Blocks event loop!
  const hash = crypto.createHash('sha256').update(buffer).digest('hex')
  return hash
}
```

**Fix**:
```typescript
// src/api/upload.ts
export async function handleUpload(file: File) {
  const buffer = await fs.promises.readFile(file.path)  // Non-blocking
  const hash = crypto.createHash('sha256').update(buffer).digest('hex')
  return hash
}
```

### Serial I/O
- **Sequential API calls**: Waiting for each call before starting next
- **Sequential DB queries**: Could be run in parallel
- **Waterfall requests**: Frontend making serial requests
- **Missing concurrency**: Not utilizing Promise.all()

**Example MED**:
```typescript
// src/services/data-service.ts - MED: Serial I/O!
export async function fetchAllData(userId: string) {
  const user = await fetchUser(userId)        // Wait
  const posts = await fetchPosts(userId)      // Wait
  const comments = await fetchComments(userId)  // Wait
  return { user, posts, comments }
}

// 3 sequential calls = 300ms total (if each is 100ms)
```

**Fix**:
```typescript
// src/services/data-service.ts
export async function fetchAllData(userId: string) {
  // Parallel execution
  const [user, posts, comments] = await Promise.all([
    fetchUser(userId),
    fetchPosts(userId),
    fetchComments(userId)
  ])
  return { user, posts, comments }
}

// 3 parallel calls = 100ms total (fastest call time)
```

## 5. Frontend Performance

### Rendering Performance
- **Unnecessary re-renders**: Components re-rendering without changes
- **Missing memoization**: Expensive computations on every render
- **Large DOM updates**: Updating thousands of elements
- **No virtualization**: Rendering 10,000 list items

**Example HIGH**:
```typescript
// src/components/UserList.tsx - HIGH: Renders all 10K users!
export function UserList({ users }: { users: User[] }) {
  return (
    <div>
      {users.map(user => (
        <UserCard key={user.id} user={user} />  // HIGH: 10K components!
      ))}
    </div>
  )
}
```

**Fix**:
```typescript
// src/components/UserList.tsx
import { FixedSizeList } from 'react-window'

export function UserList({ users }: { users: User[] }) {
  return (
    <FixedSizeList
      height={600}
      itemCount={users.length}
      itemSize={80}
      width="100%"
    >
      {({ index, style }) => (
        <div style={style}>
          <UserCard user={users[index]} />  // Only renders visible items
        </div>
      )}
    </FixedSizeList>
  )
}
```

### Bundle Size
- **Importing entire libraries**: `import _ from 'lodash'` instead of specific functions
- **No code splitting**: Single bundle for entire app
- **Unoptimized images**: Large image files
- **Missing tree shaking**: Dead code included in bundle

## 6. Caching Issues

### Cache Misses
- **Cache too small**: LRU cache size too low, frequent evictions
- **Wrong cache key**: Cache key doesn't capture uniqueness
- **No cache warming**: Cold start every time
- **Stale cache**: Not invalidating on updates

### Over-caching
- **Caching volatile data**: Data that changes frequently
- **Caching personalized data**: Per-user data cached globally
- **Cache stampede**: All requests miss cache simultaneously
- **Memory pressure**: Cache consuming too much memory

## 7. Concurrency Issues

### Race Conditions
- **Read-modify-write**: Multiple concurrent updates to same resource
- **Double spending**: Inventory/balance checks
- **Missing locks**: Concurrent access to shared state
- **Optimistic locking missing**: No version checking on updates

### Thread Pool Exhaustion
- **Blocking thread pool**: Long-running tasks blocking workers
- **No backpressure**: Accepting unlimited concurrent requests
- **Resource starvation**: All connections to DB consumed
- **Missing timeouts**: Requests hanging forever

## 8. Network Optimization

### Payload Size
- **Large JSON responses**: Sending unnecessary data
- **No compression**: Not using gzip/brotli
- **Chatty API**: Many small requests instead of one batch
- **Missing pagination**: Returning thousands of records

**Example MED**:
```typescript
// src/api/users.ts - MED: Returns all fields!
export async function getUsers() {
  return db.users.findMany({
    select: {  // MED: Selecting everything!
      id: true,
      email: true,
      password: true,  // Sending password hash to client!
      firstName: true,
      lastName: true,
      bio: true,
      profileImage: true,
      createdAt: true,
      updatedAt: true,
      lastLogin: true
      // ... 20 more fields
    }
  })
}
```

**Fix**:
```typescript
// src/api/users.ts
export async function getUsers() {
  return db.users.findMany({
    select: {  // Only fields needed by client
      id: true,
      email: true,
      firstName: true,
      lastName: true,
      profileImage: true
    },
    take: 50  // Paginate
  })
}
```

## 9. Profiling & Measurement

### Missing Metrics
- **No performance tracking**: No timing measurements
- **No slow query logging**: Can't identify slow database queries
- **No APM**: No distributed tracing
- **No client-side metrics**: No Real User Monitoring

### Premature Optimization
- **Optimizing cold paths**: Optimizing rarely-used code
- **Micro-optimizations**: Shaving nanoseconds in non-critical code
- **No profiling data**: Guessing where bottlenecks are
- **Over-engineering**: Complex solutions for non-problems

# WORKFLOW

## Step 0: Infer SESSION_SLUG if not provided

Standard session inference from `.claude/README.md` (last entry).

## Step 1: Load session context

1. Validate `.claude/<SESSION_SLUG>/` exists
2. Read `.claude/<SESSION_SLUG>/README.md` for context
3. Check for plan to understand performance goals
4. Look for existing profiling data

## Step 2: Determine review scope

From SCOPE, TARGET, PATHS:

1. **PATHS** (if not provided, default):
   - Hot paths: `src/api/**`, `src/services/**`
   - Frontend: `src/components/**`, `src/pages/**`
   - Database: `src/queries/**`, `src/models/**`

## Step 3: Gather code and metrics

Use Bash + Grep:
```bash
# Find nested loops
grep -rn "for.*for" src/

# Find database queries in loops
grep -rn "await.*findOne\|query" src/ | grep -B5 "for\|map"

# Find synchronous operations
grep -rn "Sync(" src/

# Find SELECT *
grep -rn "SELECT \*" src/

# Find large array operations
grep -rn "\.map\|\.filter\|\.reduce" src/
```

## Step 4: Scan for performance issues

For each checklist category:

### Algorithmic Scan
- Find nested loops
- Check Big-O complexity
- Look for unnecessary iterations
- Find sort/search inefficiencies

### Database Scan
- Find N+1 queries
- Check for missing indexes
- Look for SELECT *
- Verify pagination exists

### Memory Scan
- Find unbounded caches
- Check for memory leaks
- Look for large allocations
- Verify proper cleanup

### I/O Scan
- Find blocking operations
- Check for serial I/O
- Look for missing parallelization
- Verify timeouts exist

### Frontend Scan (if applicable)
- Check rendering efficiency
- Look for missing memoization
- Verify virtualization for long lists
- Check bundle size

## Step 5: Assess each finding

For each issue:

1. **Severity**:
   - BLOCKER: None (performance rarely blocks)
   - HIGH: O(n²) in hot path, N+1 queries, memory leaks
   - MED: Blocking I/O, missing caching, large payloads
   - LOW: Micro-optimizations in cold paths
   - NIT: Premature optimizations

2. **Confidence**:
   - High: Clear inefficiency with complexity analysis
   - Med: Likely issue, depends on data size
   - Low: Theoretical concern, needs profiling

3. **Impact estimate**:
   - Latency impact (ms saved)
   - Throughput impact (req/s improvement)
   - Resource usage (memory/CPU reduction)

## Step 6: Generate review report

Create `.claude/<SESSION_SLUG>/reviews/review-performance-{YYYY-MM-DD}.md`

## Step 7: Update session README

Standard artifact tracking update.

## Step 8: Output summary

Print summary with critical findings.

# OUTPUT FORMAT

Create `.claude/<SESSION_SLUG>/reviews/review-performance-{YYYY-MM-DD}.md`:

```markdown
---
command: /review:performance
session_slug: {SESSION_SLUG}
scope: {SCOPE}
completed: {YYYY-MM-DD}
---

# Performance Review

**Scope:** {Description of code reviewed}
**Reviewer:** Claude Performance Review Agent
**Date:** {YYYY-MM-DD}

## Summary

{Overall performance assessment}

**Severity Breakdown:**
- BLOCKER: {count}
- HIGH: {count} (O(n²) in hot paths, N+1 queries, memory leaks)
- MED: {count} (Blocking I/O, missing caching)
- LOW: {count} (Cold path optimizations)
- NIT: {count} (Premature optimizations)

**Performance Health:**
- Algorithm complexity: {PASS/FAIL}
- Database efficiency: {PASS/FAIL}
- Memory management: {PASS/FAIL}
- I/O operations: {PASS/FAIL}
- Caching strategy: {PASS/FAIL}

## Findings

### Finding 1: O(n²) Complexity in User Lookup [HIGH]

**Location:** `src/api/users.ts:45`
**Category:** Algorithmic Complexity
**Hot Path:** Yes (user-facing API endpoint)

**Issue:**
Nested loop to match users with posts results in O(n²) complexity. With 1000 users and 10000 posts, this performs 10 million iterations.

**Evidence:**
```typescript
export function getUsersWithPosts(users: User[], posts: Post[]) {
  return users.map(user => ({
    ...user,
    posts: posts.filter(post => post.userId === user.id)  // O(n²)
  }))
}
```

**Performance Impact:**
- **Complexity**: O(n × m) = O(10000000) with realistic data
- **Estimated latency**: ~500ms for 1000 users
- **Scalability**: Linear growth becomes quadratic

**Fix:**
```typescript
export function getUsersWithPosts(users: User[], posts: Post[]) {
  // Build index - O(n)
  const postsByUser = new Map<string, Post[]>()
  for (const post of posts) {
    if (!postsByUser.has(post.userId)) {
      postsByUser.set(post.userId, [])
    }
    postsByUser.get(post.userId)!.push(post)
  }

  // Lookup - O(n)
  return users.map(user => ({
    ...user,
    posts: postsByUser.get(user.id) || []
  }))
}
```

**Expected Improvement:**
- **New complexity**: O(n + m) = O(11000)
- **Speedup**: ~900x faster
- **Estimated latency**: <1ms

---

{Continue for all findings}

## Recommendations

### Immediate Actions (HIGH)
1. **Fix O(n²) in getUsersWithPosts**: Use Map for O(1) lookups
2. **Fix N+1 query in getPostsWithAuthors**: Add eager loading
3. **Fix memory leak in cache**: Implement LRU eviction
4. **Add database index on users.email**: Query taking 200ms without index

### Performance Improvements (MED)
1. **Parallelize data fetching**: Use Promise.all() in fetchAllData
2. **Add response compression**: Enable gzip for API responses
3. **Implement pagination**: Limit user list to 50 items
4. **Add virtualization**: Use react-window for long lists

### Monitoring & Profiling (LOW)
1. **Add performance tracking**: Instrument hot paths with timing
2. **Enable slow query logging**: Log queries >100ms
3. **Add APM**: Implement distributed tracing
4. **Profile in production**: Collect real user data

## Performance Impact Summary

| Finding | Current | After Fix | Improvement |
|---------|---------|-----------|-------------|
| getUsersWithPosts O(n²) | 500ms | <1ms | 500x faster |
| N+1 posts query | 1.2s (101 queries) | 80ms (1 query) | 15x faster |
| Memory leak in cache | Unbounded | 10MB cap | Memory stable |
| Missing email index | 200ms | 2ms | 100x faster |

## Profiling Recommendations

To validate these findings, profile:
1. **getUsersWithPosts** with 1000 users × 10K posts
2. **Database queries** with query logging enabled
3. **Memory usage** over 24 hours
4. **API latency** p50/p95/p99 metrics

*Review completed: {YYYY-MM-DD HH:MM}*
*Session: [{SESSION_SLUG}](../README.md)*
```

# SUMMARY OUTPUT

After creating review, print:

```markdown
# Performance Review Complete

## Review Location
Saved to: `.claude/{SESSION_SLUG}/reviews/review-performance-{YYYY-MM-DD}.md`

## Merge Recommendation
**{BLOCK | REQUEST_CHANGES | APPROVE_WITH_COMMENTS | APPROVE}**

## Critical Issues (HIGH)
{List of HIGH findings with estimated impact}

## Performance Summary
- **Hottest issue**: {Description with complexity/latency}
- **Estimated speedup**: {X}x faster after fixes
- **Memory impact**: {Leak/allocation issues}
- **Database impact**: {Query count reduction}

## Quick Wins (Biggest Impact / Least Effort)
1. {Finding with best ROI - e.g., "Add index on users.email (2 min work, 100x speedup)"}
2. {Second best ROI}
3. {Third best ROI}

## Profiling Recommended
{List areas that need measurement before optimization}
```
